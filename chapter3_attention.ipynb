{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs5GGCBIna3N/fHcf2jbQT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhoangce/llm_from_scratch/blob/main/chapter3_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Attending to different parts of the input with self-attention\n",
        "\n",
        "**THE \"SELF\" IN SELF-ATTENTION**\n",
        "\n",
        "In self-attention, the \"self\" refers to the mechanism's ability to compute attention weights by relating different positions within a single input sequence. It asseses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image.\n",
        "\n",
        "This is in constrast to traditional attention mechanisms, where the focus is on the relationships between elements of two different sequences, such as in sequence-to-sequence models where the attention might be between an input sequence and an output sequence."
      ],
      "metadata": {
        "id": "z0JCmi4rh2wR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1 A simple self-attention mechanism without trainable weights\n",
        "\n",
        "The goal of self-attention is to compute a context vector for each input element that combines information from all other input elements. A *context vector* can be interpreted as an enriched embedding vector/representation of each element in an input sequence (like a sentence) by incorporating information from all other elements in the sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "LJJ8rlRRmv03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "     [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "     [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "     [0.22, 0.58, 0.33], # with     (x^4)\n",
        "     [0.77, 0.25, 0.10], # one      (x^5)\n",
        "     [0.05, 0.80, 0.55]  # step     (x^6)\n",
        "     ]\n",
        ")\n",
        "\n",
        "\n",
        "# calculate intermediate attention scores between\n",
        "# query token and each input token with dot product\n",
        "query = inputs[1]\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(x_i, query)\n",
        "\n",
        "attn_scores_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hoFGwstm096",
        "outputId": "5c02693e-5217-49c6-a407-1ca43659e160"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dot Product**: Beyond an operation as a mathematical tool that combines two vectors to yielda scaler value, the dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot product indicates a greater degree of alignment of similarity between the vectors. In the context of self-attention mechanisms, the dot product determines the extend to which each element in a sequence focuses on, or \"attend to\", any other elements: the higher the dot product, the higher the similarity and attention score between the two elements.\n",
        "\n",
        "**Mental model to remember**\n",
        "\n",
        "* Vectors = meanings\n",
        "\n",
        "* Dot product = alignment of meanings\n",
        "\n",
        "* High alignment = strong attention\n",
        "\n",
        "* Self-attention = each token asking “who is most relevant to me?”"
      ],
      "metadata": {
        "id": "3E868eCwpmpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize attention scores for  interpretation and stability\n",
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "print(f'Attention weights: {attn_weights_2_tmp}')\n",
        "print(f'Sum: {attn_weights_2_tmp.sum()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmT3LiFDq5pA",
        "outputId": "f38959e1-21c7-45b7-8d06-2dd23018698a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: 1.0000001192092896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize using softmax\n",
        "def softmax_naive(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "print(f'Attention weights: {attn_weights_2_naive}')\n",
        "print(f'Sum: {attn_weights_2_naive.sum()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKj1GFbJrYlC",
        "outputId": "8593963e-a2f7-400d-cf2e-5dbf7eb46c77"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate context vector by multiplying embedded input tokens\n",
        "# with corresponding attention weights, then sum results\n",
        "query = inputs[1]\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  context_vec_2 += attn_weights_2_tmp[i] * x_i\n",
        "\n",
        "context_vec_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHoNkKSlr_VJ",
        "outputId": "f9b7ef1d-c77b-4a13-abff-7c40b0c4fe6f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4355, 0.6451, 0.5680])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = torch.matmul(attn_weights_2_tmp, inputs)\n",
        "context_vec_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9705mDesrQd",
        "outputId": "240ad00c-386e-495d-dc04-2978fc4c9891"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4355, 0.6451, 0.5680])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 Computing attention weights for all input tokens"
      ],
      "metadata": {
        "id": "OWj5tLSktAlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "attn_scores = torch.matmul(inputs, inputs.T) # same as using @\n",
        "attn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er2RWhZ-t-bB",
        "outputId": "fb687b9b-daf9-495e-c954-dbc6b502f22d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
              "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
              "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
              "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
              "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
              "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs @ inputs.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V2SSFg3wxR8",
        "outputId": "bd45325e-fdac-495f-e830-e776abd473be"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
              "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
              "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
              "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
              "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
              "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting dim=-1 apply normalization along last dim,\n",
        "# e.g: (rows, columns) -> normalized across columns\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBsWRDyxw-VO",
        "outputId": "a0f36674-f9de-41c0-f58e-8927b4500d80"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
              "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
              "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
              "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
              "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
              "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute context vectors for all inputs\n",
        "all_context_vecs = attn_weights @ inputs\n",
        "all_context_vecs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd0qPyRsxnim",
        "outputId": "3bcfb9fd-1104-43c9-a9dd-88bbc6578102"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4421, 0.5931, 0.5790],\n",
              "        [0.4419, 0.6515, 0.5683],\n",
              "        [0.4431, 0.6496, 0.5671],\n",
              "        [0.4304, 0.6298, 0.5510],\n",
              "        [0.4671, 0.5910, 0.5266],\n",
              "        [0.4177, 0.6503, 0.5645]])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Implementing self-attention with trainable weights"
      ],
      "metadata": {
        "id": "xLdTtxDexxTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.1 Computing the attention weights step by step"
      ],
      "metadata": {
        "id": "iqTJ6DAP7FVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1] # input embedding size\n",
        "d_out = 2 # output embedding size"
      ],
      "metadata": {
        "id": "L9UU8RJz8yR0"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Parameter\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# initialize weight matrices\n",
        "W_query = Parameter(torch.rand(d_in, d_out),\n",
        "                     requires_grad=False)\n",
        "W_key = Parameter(torch.rand(d_in, d_out),\n",
        "                   requires_grad=False)\n",
        "W_value = Parameter(torch.rand(d_in, d_out),\n",
        "                     requires_grad=False)"
      ],
      "metadata": {
        "id": "-1DR9lrR87mg"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute query, key, value vectors\n",
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "query_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi4RT7kE9dca",
        "outputId": "ba77301b-2b05-4e6b-aa43-61736fdbfacf"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4306, 1.4551])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get all keys and values\n",
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(f'keys.shape: {keys.shape}')\n",
        "print(f'values.shape: {values.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGOqVYo59qL0",
        "outputId": "80678a02-5b98-47a8-fd1d-84e5b2d82e21"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention score**: dot-product computation between input elements using *query* and *key* by transforming inputs via respective weight matrices."
      ],
      "metadata": {
        "id": "O5T4D927-lmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute attention score omega_22\n",
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "attn_score_22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg96jSiX-JeF",
        "outputId": "55953175-8be7-44b1-9d83-da923909d249"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.8524)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attention scores for query 2 with all inputs\n",
        "attn_scores_2 = query_2 @ keys.T\n",
        "attn_scores_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzm0Y2No-dNH",
        "outputId": "5b0d2c1b-716b-4372-f6b9-0429f5db45cc"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scale attention scores and apply softmax\n",
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5,\n",
        "                               dim=-1)\n",
        "attn_weights_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT3ydG00_Nb8",
        "outputId": "a4fa0ab5-6e58-4e8c-aa8d-414aea9d1ab6"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THE RATIONALE BEHIND SCALED-DOT PRODUCT ATTENTION**\n",
        "\n",
        "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function resulting in gradient nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
        "\n",
        "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention."
      ],
      "metadata": {
        "id": "QSNCMCAm_3vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute context vectors\n",
        "context_vec_2 = attn_weights_2 @ values\n",
        "context_vec_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc-PHGh4CLic",
        "outputId": "b76fa1c4-72be-4478-f742-ee6394754586"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3061, 0.8210])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.2 Implementing a compact self-attention Python class"
      ],
      "metadata": {
        "id": "13gMn9RrC20F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = x @ self.W_key\n",
        "    queries = x @ self.W_query\n",
        "    values = x @ self.W_value\n",
        "    attn_scores = queries @ keys.T # omega\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "    )\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        ""
      ],
      "metadata": {
        "id": "JshtOj_eM3EM"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "sa_v1(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Anfcaf6N2Go",
        "outputId": "45ba883b-c97f-4530-cd65-26a63027e16a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2996, 0.8053],\n",
              "        [0.3061, 0.8210],\n",
              "        [0.3058, 0.8203],\n",
              "        [0.2948, 0.7939],\n",
              "        [0.2927, 0.7891],\n",
              "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "    )\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        ""
      ],
      "metadata": {
        "id": "ebv657rPOQN6"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "sa_v2(inputs) # callable-object for efficiency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9GEAW0BWw7x",
        "outputId": "822d1145-da25-4946-e4a6-613ef654435c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0739,  0.0713],\n",
              "        [-0.0748,  0.0703],\n",
              "        [-0.0749,  0.0702],\n",
              "        [-0.0760,  0.0685],\n",
              "        [-0.0763,  0.0679],\n",
              "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa_v2.W_query, sa_v1.W_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKa2CegfW3MW",
        "outputId": "fe0fd7b9-cad0-439e-b3ad-aff2fe82bc14"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Linear(in_features=3, out_features=2, bias=False),\n",
              " Parameter containing:\n",
              " tensor([[0.2961, 0.5166],\n",
              "         [0.2517, 0.6886],\n",
              "         [0.0740, 0.8665]], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign weights from nn.Linear instance to nn.Parameters obj\n",
        "sa_v1.W_query.data = sa_v2.W_query.weight.data.T\n",
        "sa_v1.W_key.data = sa_v2.W_key.weight.data.T\n",
        "sa_v1.W_value.data = sa_v2.W_value.weight.data.T\n",
        "\n",
        "sa_v1(inputs), sa_v2(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGByPSAPZCh2",
        "outputId": "a2eb04fe-77f8-406c-e8f2-4f36c838b6f7"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0739,  0.0713],\n",
              "         [-0.0748,  0.0703],\n",
              "         [-0.0749,  0.0702],\n",
              "         [-0.0760,  0.0685],\n",
              "         [-0.0763,  0.0679],\n",
              "         [-0.0754,  0.0693]], grad_fn=<MmBackward0>),\n",
              " tensor([[-0.0739,  0.0713],\n",
              "         [-0.0748,  0.0703],\n",
              "         [-0.0749,  0.0702],\n",
              "         [-0.0760,  0.0685],\n",
              "         [-0.0763,  0.0679],\n",
              "         [-0.0754,  0.0693]], grad_fn=<MmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Hiding future words with casual attention\n",
        "\n",
        "**Casual attention**: also known as *masked attention, restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores. This is in contrast to the standard self-attention mechanism, which allows access to the enture input sequence at once."
      ],
      "metadata": {
        "id": "CAGdUK8wZoEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.1 Applying a casual attention mask"
      ],
      "metadata": {
        "id": "6z3mKj3lbHRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys.shape[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7sqv0Lndnkh",
        "outputId": "c5d4b3e2-ff31-48df-a5c2-47399c943ee3"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys.shape[-1]**0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJX55gwfdqoZ",
        "outputId": "ca628db4-7031-499c-a13a-931302c8c1a9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4142135623730951"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute attention weights using softmax\n",
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(\n",
        "    attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        ")\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOOKF_GgcXRK",
        "outputId": "cc1ad779-5e73-4085-dad2-62da2948b9ee"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
              "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
              "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
              "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
              "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
              "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6zLoZcydfAG",
        "outputId": "bb4e2430-f9d2-49dc-b7d7-28d6401860b2"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
              "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
              "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
              "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
              "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
              "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create mask of value 0 above diagonal\n",
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length,\n",
        "                                    context_length))\n",
        "mask_simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-A7KKy8dsDH",
        "outputId": "73294087-8e62-4060-d7ff-ea782cdc4257"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zero out values above diagonal\n",
        "masked_simple = attn_weights * mask_simple\n",
        "masked_simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX3IpL7GhcJv",
        "outputId": "f5f1b668-962c-4987-c56e-73e46f4ec719"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
              "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
              "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# renormalize attention weights to sum to 1 per row\n",
        "row_sums = masked_simple.sum(dim=-1, keepdims=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "masked_simple_norm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_7_rTHBhvw3",
        "outputId": "e6b1fca6-71dd-427c-b659-e735e19767f0"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
              "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
              "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFORMATION LEAKAGE**\n",
        "\n",
        "When we apply a mask and then renormalize the attention weights, it might initially appear that information from future tokens (which we intend to mask) could still influence the current token because their values are part of the softmax calculation. However, the key insight is that when we renormalize the attention weights after masking, what we're essentially doing is recalculating the softmax over a smaller subset (since maksed positions don't contribute to the softmax value).\n",
        "\n",
        "The mathematical elegance of softmax is that despite initially including all positions in the denominator, after masking and renormalizing, the effect of the masked positions is nullified - they don't contribute to the softmax score in any meaningful way.\n",
        "\n",
        "In simpler terms, after masking and renormalization, the distribution of attention weights is as if it was calculated only among the unmasked positions to begin with. This ensures there's no information leakage from future (or otherwise masked) tokens as we intended."
      ],
      "metadata": {
        "id": "lXhPIkF8iDnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax converts its inputs into a probability distribution. When negative infinity values are present in a row, the softmax function treats them as zero probability (Mathematically, this is because e^-inf approaches 0).\n",
        "\n",
        "We can implement this more efficient masking \"trick\" by creating a mask with 1s above the diagonal and then replacing these 1s with negative infinity values:\n"
      ],
      "metadata": {
        "id": "eheeJy4_jSa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length),\n",
        "                  diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFI3SRebj408",
        "outputId": "d8301daa-1a47-41a8-c934-eded1e4abdb5"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
              "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
              "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
              "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
              "       grad_fn=<MaskedFillBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply softmax to these results\n",
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5,\n",
        "                             dim=-1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIMYSSw3kESJ",
        "outputId": "16510d2d-f798-4c37-8018-7ac376a02831"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
              "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
              "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.2 Masking additional attention weights with dropout\n",
        "\n",
        "In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors. Here we will apply the dropout mask after computing the attention weights because it's more common in practice."
      ],
      "metadata": {
        "id": "5i92ApAtkW4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply dropout rate of 50%\n",
        "torch.manual_seed(123)\n",
        "\n",
        "dropout = nn.Dropout(0.5)\n",
        "example = torch.ones(6, 6)\n",
        "dropout(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFF2iRQzlCVz",
        "outputId": "cd214c6f-96e2-4c97-f495-a4fbf25b781d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 2., 0., 2., 2., 0.],\n",
              "        [0., 0., 0., 2., 0., 2.],\n",
              "        [2., 2., 2., 2., 0., 2.],\n",
              "        [0., 2., 2., 0., 0., 2.],\n",
              "        [0., 2., 0., 2., 0., 2.],\n",
              "        [0., 2., 2., 2., 2., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When applying aropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of $1/0.5 = 2$. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases."
      ],
      "metadata": {
        "id": "tsNryeQ_lrWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4oqlZ_ImjlH",
        "outputId": "222d4366-c9f9-41d8-f3cc-fcf457b11b1f"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
              "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.3 Implementing a compact casual attention class"
      ],
      "metadata": {
        "id": "4MoNVtDqmrVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simulate batch inputs\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7-KrLxvoPB4",
        "outputId": "b4b7f9ce-3b31-4e22-fa6b-43b734d74e36"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  1. Adds dropout layer\n",
        "  2. not strictly necessary but offer advantages: buffer automatically moved to appropriate device along with model\n",
        "  3. transposes dimensions 1 and 2, keep same batch dimension\n",
        "  4. in place operation for memory efficiency\n",
        "  \"\"\"\n",
        "  def __init__(self, d_in, d_out, context_length,\n",
        "               dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout) # 1\n",
        "    self.register_buffer( # 2\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(context_length, context_length),\n",
        "                   diagonal=1),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1, 2) # 3\n",
        "    attn_scores.masked_fill_( # 4\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
        "    )\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "    )\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "IEDfSm66ohCe"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "context_vecs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jThRzmPiqDRU",
        "outputId": "76442b24-a725-48ec-9aa9-2d35d74e63d9"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Extending single-head attention to multi-head attention\n",
        "\n",
        "The term \"multi-head\" refers to dividing the attention mechanism into multiple \"heads\", each operating independently. In this context, a single casual attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially."
      ],
      "metadata": {
        "id": "vVxhO_NurgoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6.1 Stacking multiple single-head attention layers\n",
        "\n",
        "The main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections - the results of multipling the input data (like query, key, and value vectors in attention mechanisms) by a weight matrix."
      ],
      "metadata": {
        "id": "hU2MKntipc76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length,\n",
        "               dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CausalAttention(\n",
        "            d_in, d_out, context_length, dropout, qkv_bias\n",
        "        )\n",
        "        for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)\n"
      ],
      "metadata": {
        "id": "tpUyGmROp_5d"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1] # number of tokens\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in, d_out, context_length, 0.0, num_heads=2\n",
        ")\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(f'context_vecs.shape: {context_vecs.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCUGkYbsrdaS",
        "outputId": "73ed7b62-56f2-4b24-e216-204a8f935d5d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in, 1, context_length, 0.0, num_heads=2\n",
        ")\n",
        "context_vecs = mha(batch)\n",
        "context_vecs, context_vecs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgDrRjkfsPB9",
        "outputId": "6c8f9350-f86b-44c1-a6d3-e768f7e14466"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0189, 0.2729],\n",
              "          [0.2181, 0.3037],\n",
              "          [0.2804, 0.3125],\n",
              "          [0.2830, 0.2793],\n",
              "          [0.2476, 0.2541],\n",
              "          [0.2748, 0.2513]],\n",
              " \n",
              "         [[0.0189, 0.2729],\n",
              "          [0.2181, 0.3037],\n",
              "          [0.2804, 0.3125],\n",
              "          [0.2830, 0.2793],\n",
              "          [0.2476, 0.2541],\n",
              "          [0.2748, 0.2513]]], grad_fn=<CatBackward0>),\n",
              " torch.Size([2, 6, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6.2 Implementing multi-head attention with weight splits"
      ],
      "metadata": {
        "id": "j-ORaeZat9wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Integrates the multi-head functionality within a single\n",
        "  class. It splits the input into multiple heads by reshaping the projected query, key, value tensor and then combines the\n",
        "  results from these heads after computing attention.\n",
        "\n",
        "  1. Reduces the projection dim to match the desired output dim\n",
        "  2. Uses a Linear layer to combine head outputs\n",
        "  3. Tensor shape: (b, num_tokens, d_out)\n",
        "  4. Implicitly split the matrix by adding a num_heads\n",
        "  dimension. Then unrolls the last dim: (n, num_tokens, d_out)\n",
        "  -> (b, num_tokens, num_heads, head_dim)\n",
        "  5. Transposes from shape (b, num_tokens, num_heads, head_dim)\n",
        "  to (b, num_heads, num_tokens, head_dim)\n",
        "  6. Computes dot product for each head\n",
        "  7. Masks truncated to the number of tokens\n",
        "  8. Uses the mask to fill attention scores\n",
        "  9. Tensor shape: (n, num_tokens, n_heads, head_dim)\n",
        "  10. Combines heads, where self.d_out = self.num_heads * self.head_dim\n",
        "  11. Adds an optional linear projection\n",
        "  \"\"\"\n",
        "  def __init__(self, d_in, d_out, context_length,\n",
        "               dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \\\n",
        "        \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads # 1\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out) # 2\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(context_length, context_length),\n",
        "                   diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x) # 3\n",
        "    queries = self.W_query(x) # 3\n",
        "    values = self.W_value(x) # 3\n",
        "\n",
        "    keys = keys.view(b, num_tokens, self.num_heads,\n",
        "                      self.head_dim) # 4\n",
        "    values = values.view(b, num_tokens,\n",
        "                         self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens,\n",
        "                           self.num_heads, self.head_dim)\n",
        "\n",
        "    keys = keys.transpose(1, 2) # 5\n",
        "    queries = queries.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2, 3) # 6\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # 7\n",
        "\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf) # 8\n",
        "\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "    )\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2) # 9\n",
        "    context_vec = context_vec.contiguous().view(\n",
        "        b, num_tokens, self.d_out\n",
        "    ) # 10\n",
        "    context_vec = self.out_proj(context_vec) # 11\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "BHxQnk5Ru1th"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Illustrate batched matrix multiplication\n",
        "# shape (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
        "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
        "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
        "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
        "\n",
        "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
        "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
        "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
        "\n",
        "a.transpose(2, 3), a.transpose(2, 3).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpvNPskmH46A",
        "outputId": "faed6d33-40dd-45a8-b84f-10ab9d951a2d"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[[0.2745, 0.8993, 0.7179],\n",
              "           [0.6584, 0.0390, 0.7058],\n",
              "           [0.2775, 0.9268, 0.9156],\n",
              "           [0.8573, 0.7388, 0.4340]],\n",
              " \n",
              "          [[0.0772, 0.4066, 0.4606],\n",
              "           [0.3565, 0.2318, 0.5159],\n",
              "           [0.1479, 0.4545, 0.4220],\n",
              "           [0.5331, 0.9737, 0.5786]]]]),\n",
              " torch.Size([1, 2, 4, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a @ a.transpose(2, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIL781n6JLNg",
        "outputId": "ef19d867-466b-44c7-fe44-fde35723d06b"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1.3208, 1.1631, 1.2879],\n",
              "          [1.1631, 2.2150, 1.8424],\n",
              "          [1.2879, 1.8424, 2.0402]],\n",
              "\n",
              "         [[0.4391, 0.7003, 0.5903],\n",
              "          [0.7003, 1.3737, 1.0620],\n",
              "          [0.5903, 1.0620, 0.9912]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_head = a[0, 0, :, :]\n",
        "second_head = a[0, 1, :, :]\n",
        "first_head, second_head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXFzVljfJnUb",
        "outputId": "e8fdf170-b3e5-4ecc-8cbf-0b5069d36e79"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.2745, 0.6584, 0.2775, 0.8573],\n",
              "         [0.8993, 0.0390, 0.9268, 0.7388],\n",
              "         [0.7179, 0.7058, 0.9156, 0.4340]]),\n",
              " tensor([[0.0772, 0.3565, 0.1479, 0.5331],\n",
              "         [0.4066, 0.2318, 0.4545, 0.9737],\n",
              "         [0.4606, 0.5159, 0.4220, 0.5786]]))"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_res = first_head @ first_head.T\n",
        "second_res = second_head @ second_head.T\n",
        "first_res, second_res\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eas34i_J64V",
        "outputId": "95b655a5-9205-4fdb-f25b-b465274f3ee7"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1.3208, 1.1631, 1.2879],\n",
              "         [1.1631, 2.2150, 1.8424],\n",
              "         [1.2879, 1.8424, 2.0402]]),\n",
              " tensor([[0.4391, 0.7003, 0.5903],\n",
              "         [0.7003, 1.3737, 1.0620],\n",
              "         [0.5903, 1.0620, 0.9912]]))"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0,\n",
        "                         num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "context_vecs, context_vecs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0jK23nEKR7_",
        "outputId": "65403384-0372-437f-8d36-5641f7f335e4"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.3190, 0.4858],\n",
              "          [0.2943, 0.3897],\n",
              "          [0.2856, 0.3593],\n",
              "          [0.2693, 0.3873],\n",
              "          [0.2639, 0.3928],\n",
              "          [0.2575, 0.4028]],\n",
              " \n",
              "         [[0.3190, 0.4858],\n",
              "          [0.2943, 0.3897],\n",
              "          [0.2856, 0.3593],\n",
              "          [0.2693, 0.3873],\n",
              "          [0.2639, 0.3928],\n",
              "          [0.2575, 0.4028]]], grad_fn=<ViewBackward0>),\n",
              " torch.Size([2, 6, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_mha = MultiHeadAttention(d_in=768, d_out=768,\n",
        "                             context_length=1024,\n",
        "                             dropout=0.0,\n",
        "                             num_heads=12)\n",
        "gpt_mha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WvitPftLEr-",
        "outputId": "53d2e5ef-dee6-4723-ddf2-6744db7814db"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiHeadAttention(\n",
              "  (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "  (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "  (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_gpt = torch.rand(size=(2, 768, 768))\n",
        "gpt_mha(batch_gpt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnFGHN__L3s3",
        "outputId": "d14479a7-84b6-49fc-e1e3-9f3a94b647b2"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0929, -0.0228, -0.0636,  ..., -0.0095, -0.1502, -0.0183],\n",
              "         [-0.0523, -0.0455,  0.0185,  ...,  0.0080, -0.0402, -0.1689],\n",
              "         [-0.0413, -0.0100,  0.0419,  ..., -0.0295, -0.0688, -0.1732],\n",
              "         ...,\n",
              "         [ 0.0696,  0.0380, -0.0127,  ...,  0.0112, -0.0339, -0.0346],\n",
              "         [ 0.0695,  0.0381, -0.0128,  ...,  0.0114, -0.0340, -0.0345],\n",
              "         [ 0.0695,  0.0380, -0.0126,  ...,  0.0112, -0.0336, -0.0344]],\n",
              "\n",
              "        [[ 0.1281, -0.0059, -0.0568,  ...,  0.0641, -0.2194, -0.1864],\n",
              "         [ 0.0844,  0.0588, -0.0159,  ...,  0.0928, -0.0769, -0.0961],\n",
              "         [ 0.1025,  0.1080, -0.0090,  ...,  0.1047, -0.0564, -0.0802],\n",
              "         ...,\n",
              "         [ 0.0753,  0.0463, -0.0138,  ...,  0.0127, -0.0429, -0.0418],\n",
              "         [ 0.0752,  0.0462, -0.0139,  ...,  0.0127, -0.0428, -0.0417],\n",
              "         [ 0.0755,  0.0465, -0.0138,  ...,  0.0123, -0.0428, -0.0422]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "* Attention mechanisms transform input elements into enhanced context vector representations that incorporate information about all inputs.\n",
        "\n",
        "* A self-attention mechanism computes the context vector representation as a weighted sum over the inputs.\n",
        "\n",
        "* In a simplified attention mechanism, the attention weights are computed via dot products.\n",
        "\n",
        "* A dot product is a concise way of multiplying two vectors element-wise and then summing the products.\n",
        "\n",
        "* Matrix multiplications, while not strictly required, help us implement computations more efficiently and compactly by replacing nested *for* loops.\n",
        "\n",
        "* In self-attention mechanisms used in LLMs, also called scaled-dot product attention, we include trainable weight matrices to compute intermediate transformations of the inputs: queries, values, and keys.\n",
        "\n",
        "* When working with LLMs that read and generate text from left to right, we add a causal attention mask to prevent the LLM from acccessing futture tokens.\n",
        "\n",
        "* In addition to causual attention masks to zero-out attention weights, we can add a dropout mask to reduce overfitting in LLMs.\n",
        "\n",
        "* The attention modules in transformer-based LLMs involve multiple instances of causal attention, which is called multi-head attention.\n",
        "\n",
        "* We can create a multi-head attention module by stacking multiple instances of causal attention modules.\n",
        "\n",
        "* A more efficient way of creating multi-head attention modules involves batches matrix multiplications."
      ],
      "metadata": {
        "id": "ZqsqPkquL-P5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yuv62Nn0OP4r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}