{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8meXfHS/de0/TPZp7eG+X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhoangce/llm_from_scratch/blob/main/chapter2_preprocess_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Tokenizing Text"
      ],
      "metadata": {
        "id": "LNlIids74Tsi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "o17jrIOBxsSm",
        "outputId": "67defafa-ee1d-4a17-a05d-93d56341d2ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total num of characters: 20479\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# load text\n",
        "with open('the_verdict.txt', 'r') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "print(f'Total num of characters: {len(raw_text)}')\n",
        "raw_text[:99]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, work. this, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dov0OwZykBA",
        "outputId": "e58cebbe-05cb-4b74-e7ad-ee3961e73f8d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello,', ' ', 'work.', ' ', 'this,', ' ', 'is', ' ', 'a', ' ', 'test.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split on whitespaces (\\s), commas, and periods ([,.])\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LAPl4H7zAJ6",
        "outputId": "a0c50c9f-57ba-4511-8bc9-3f4ec08ef57a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " '',\n",
              " ' ',\n",
              " 'work',\n",
              " '.',\n",
              " '',\n",
              " ' ',\n",
              " 'this',\n",
              " ',',\n",
              " '',\n",
              " ' ',\n",
              " 'is',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 'test',\n",
              " '.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove whitespace characters\n",
        "result = [item for item in result if item.strip()]\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk6Co8csziqK",
        "outputId": "c7056cf8-7dc1-498f-f846-a592382ff27b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'work', '.', 'this', ',', 'is', 'a', 'test', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **NOTE**\n",
        "\n",
        "When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing)."
      ],
      "metadata": {
        "id": "ShSWEFjIz15y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# handles other types of punctuations and special characters\n",
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split('([,.:;?_!\"()\\']|--|\\\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQjgHbFs0asO",
        "outputId": "4ca5405b-029e-4d7c-87d7-d9d16e288a35"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply this tokenizer to the text\n",
        "preprocessed = re.split('([,.:;?_!\"()\\']|--|\\\\s)', raw_text)\n",
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "len(preprocessed), preprocessed[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imDIvOKE02BO",
        "outputId": "69bf25c2-eafe-4915-ad78-adde707b337e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4690,\n",
              " ['I',\n",
              "  'HAD',\n",
              "  'always',\n",
              "  'thought',\n",
              "  'Jack',\n",
              "  'Gisburn',\n",
              "  'rather',\n",
              "  'a',\n",
              "  'cheap',\n",
              "  'genius',\n",
              "  '--',\n",
              "  'though',\n",
              "  'a',\n",
              "  'good',\n",
              "  'fellow',\n",
              "  'enough',\n",
              "  '--',\n",
              "  'so',\n",
              "  'it',\n",
              "  'was'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting tokens into token IDs"
      ],
      "metadata": {
        "id": "1nzSpg8o1hEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of all unique tokens\n",
        "# and sort them alphabetically\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmtUb5Gt4fh-",
        "outputId": "7e8213b8-c023-49f9-e53b-7256c3450626"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1130"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YUgcP2p4-Jm",
        "outputId": "7531bf89-8867-46cb-83a6-4ff6ce35d2f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  \"\"\"\n",
        "  1. Stores the vocabulary as a class attribute for access in\n",
        "  the encode and decode method\n",
        "  2. Creates an inverse vocabulary that maps token IDs back to\n",
        "  the original text tokens\n",
        "  3. Processes input text into token IDs\n",
        "  4. Converts token IDs back into text\n",
        "  5. Removes spaces before the specified punctuation\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab # 1\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()} # 2\n",
        "\n",
        "  def encode(self, text): # 3\n",
        "    preprocessed = re.split('([,.:;?_!\"()\\']|--|\\\\s)', text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join(self.int_to_str[i] for i in ids)\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # 5\n",
        "    return text"
      ],
      "metadata": {
        "id": "KKqUB73k5OkH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VNU4uU66grK",
        "outputId": "b0e268f2-1f52-4a0e-a770-6ec561e00fcb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 56,\n",
              " 2,\n",
              " 850,\n",
              " 988,\n",
              " 602,\n",
              " 533,\n",
              " 746,\n",
              " 5,\n",
              " 1126,\n",
              " 596,\n",
              " 5,\n",
              " 1,\n",
              " 67,\n",
              " 7,\n",
              " 38,\n",
              " 851,\n",
              " 1108,\n",
              " 754,\n",
              " 793,\n",
              " 7]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oS2Ps2AW638v",
        "outputId": "0442f52c-3f2d-4866-f02c-922aefe90078"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test example\n",
        "text = \"Hello, do you like tea?\"\n",
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "v-RoOyLT7MlL",
        "outputId": "c10c9f40-ff5e-4880-c8a4-745d4e910e2c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2555061618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-941023397.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     ]\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Adding special context tokens"
      ],
      "metadata": {
        "id": "qkN7QkTeIea3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding two special tokens\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "\n",
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YkSFEDoIpno",
        "outputId": "c95cfad0-748f-46b9-8ca6-0cb9450aba4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mxIGu-5JZT8",
        "outputId": "e5e911c4-d5b3-4bd6-dd93-44fe6b985bed"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  \"\"\"\n",
        "  1. Replaces unknown words by <|unk|> tokens\n",
        "  2. Replaces spaces before the specified punctuations\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split('([,.:;?_!\"()\\']|--|\\\\s)', text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    preprocessed = [item if item in self.str_to_int\n",
        "                    else \"<|unk|>\" for item in preprocessed] # 1\n",
        "\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join(self.int_to_str[i] for i in ids)\n",
        "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "pUSa1oC1JiXH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yde1nlxoK7Ig",
        "outputId": "2433dbe5-9d0c-45ae-bd11-2dfbc896311b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBCF_4F0LOW1",
        "outputId": "d4f5417f-82bb-4838-9900-95432f1b6844"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IGz2i5ZALXgU",
        "outputId": "d4d65e85-f07a-438b-fd81-7530c8a6c283"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Byte pair encoding"
      ],
      "metadata": {
        "id": "xcQFqjJML5RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken -q"
      ],
      "metadata": {
        "id": "XBcbUlV9MexE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tiktoken.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fuQdzv_6Mlad",
        "outputId": "9723ae2f-60a9-4dd6-aff3-898b1b6d0d90"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "PRQEJQHRNrjZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "integers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlu9XkuZNP1H",
        "outputId": "e717ae9d-ddba-4a4e-e32b-bcc327123c61"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15496,\n",
              " 11,\n",
              " 466,\n",
              " 345,\n",
              " 588,\n",
              " 8887,\n",
              " 30,\n",
              " 220,\n",
              " 50256,\n",
              " 554,\n",
              " 262,\n",
              " 4252,\n",
              " 18250,\n",
              " 8812,\n",
              " 2114,\n",
              " 286,\n",
              " 617,\n",
              " 34680,\n",
              " 27271,\n",
              " 13]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string = tokenizer.decode(integers)\n",
        "string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NC78VuzSNljH",
        "outputId": "ec3bce68-5607-4981-9b1e-318e1544d6c4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"Akwirw ier\"\n",
        "tokenizer.encode(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--d1SmCNN0Td",
        "outputId": "ccd52e47-e6b9-4208-c2f1-fbd90a4fe447"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[33901, 86, 343, 86, 220, 959]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WU6JmHb_R09C",
        "outputId": "37dc2b4f-bf2f-468a-82f9-1406d98df3d0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Akwirw ier'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Data sampling with a sliding window"
      ],
      "metadata": {
        "id": "VDE-CZl0R4Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize \"The Verdict\" with BPE tokenizer\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "with open('the_verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "len(enc_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQALuGcMSDXv",
        "outputId": "5dfea2e0-f658-40df-8469-6768bcfd9198"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5145"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "0ZhUW5I6SkDY"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context_size determines how many tokens are in input\n",
        "context_size = 4\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f'x: {x}')\n",
        "print(f'y:      {y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SkLI0OlSwmY",
        "outputId": "de40f3d7-a444-4910-86a9-64490ee85ab9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create next-word prediction tasks\n",
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(context, '------>', desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0jGKFXRTIaD",
        "outputId": "fd0b8bd4-2f63-454d-fc16-6eb43dd79be0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ------> 4920\n",
            "[290, 4920] ------> 2241\n",
            "[290, 4920, 2241] ------> 287\n",
            "[290, 4920, 2241, 287] ------> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert token IDs into text\n",
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(tokenizer.decode(context), '----->', tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03aYFzE6TdwA",
        "outputId": "96ecea7c-67a3-4841-b215-89fee0ae4f1a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ----->  established\n",
            " and established ----->  himself\n",
            " and established himself ----->  in\n",
            " and established himself in ----->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  \"\"\"\n",
        "  1. Tokenizes the entire text\n",
        "  2. Uses a sliding window to chunk the book into overlapping sequences of max_length\n",
        "  3. Returns the total number of rows in the dataset\n",
        "  4. Returns a single row from the dataset\n",
        "  \"\"\"\n",
        "  def __init__(self, text, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(text) # 1\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride): # 2\n",
        "      input_chunk = token_ids[i : i+max_length]\n",
        "      target_chunk = token_ids[i+1 : i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        ""
      ],
      "metadata": {
        "id": "HS9fr_kZULTF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(text, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True,\n",
        "                         drop_last=True, num_workers=0):\n",
        "  \"\"\"\n",
        "  1. Initializes the tokenizer\n",
        "  2. Creates dataset\n",
        "  3. drop_last=True drops the last batch if it is shorter than\n",
        "   the specified batch_size to prevent loss spikes during training\n",
        "  \"\"\"\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')\n",
        "  dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader\n",
        "\n",
        "with open('the_verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=1,\n",
        "                                  max_length=4, stride=1,\n",
        "                                  shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "first_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb_veU3SWPB6",
        "outputId": "52f7f98d-e2dc-472e-c15f-4425df5c0330"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8,\n",
        "                                  max_length=8, stride=4,\n",
        "                                  shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV5xja4PXPa_",
        "outputId": "3f3c4e05-3bb2-4a43-cc43-fed924c5d27e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
            "        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
            "        [10899,  2138,   257,  7026, 15632,   438,  2016,   257],\n",
            "        [15632,   438,  2016,   257,   922,  5891,  1576,   438],\n",
            "        [  922,  5891,  1576,   438,   568,   340,   373,   645],\n",
            "        [  568,   340,   373,   645,  1049,  5975,   284,   502],\n",
            "        [ 1049,  5975,   284,   502,   284,  3285,   326,    11],\n",
            "        [  284,  3285,   326,    11,   287,   262,  6001,   286]])\n",
            "Targets: tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899],\n",
            "        [ 3619,   402,   271, 10899,  2138,   257,  7026, 15632],\n",
            "        [ 2138,   257,  7026, 15632,   438,  2016,   257,   922],\n",
            "        [  438,  2016,   257,   922,  5891,  1576,   438,   568],\n",
            "        [ 5891,  1576,   438,   568,   340,   373,   645,  1049],\n",
            "        [  340,   373,   645,  1049,  5975,   284,   502,   284],\n",
            "        [ 5975,   284,   502,   284,  3285,   326,    11,   287],\n",
            "        [ 3285,   326,    11,   287,   262,  6001,   286,   465]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Creating token embeddings"
      ],
      "metadata": {
        "id": "ZptAk0lqYBqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# create embedding layer initialized with random values of shape\n",
        "# (vocab_size, output_dim)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "embedding_layer.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV44Ksk5YQhu",
        "outputId": "e321a3ed-4f25-45a8-c0ae-8bbbe1b3ed7c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.3374, -0.1778, -0.1690],\n",
              "        [ 0.9178,  1.5810,  1.3010],\n",
              "        [ 1.2753, -0.2010, -0.1606],\n",
              "        [-0.4015,  0.9666, -1.1481],\n",
              "        [-1.1589,  0.3255, -0.6315],\n",
              "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply to a token Id to obtain embedding vector/look up idx\n",
        "embedding_layer(torch.tensor([3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWE0uQQtY9Lm",
        "outputId": "ed4da097-b290-40fb-b42b-690acbc394a9"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwDd_JjxZaa7",
        "outputId": "c6042d48-c24a-4bc8-f26e-0bb6366b230b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2753, -0.2010, -0.1606],\n",
              "        [-0.4015,  0.9666, -1.1481],\n",
              "        [-2.8400, -0.7849, -1.4096],\n",
              "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Encoding word positions\n",
        "\n",
        "A minor shortcoming of LLMs is that their self-attention mechanism doesn't have a notion of position or order for the tokens within a sequence. the way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence.\n",
        "\n",
        "In principle, the deterministic, position-independent embedding of the token ID is good for reproducibility purposes. However, since the self-attention mechanism of LLMs itself is also position-agnostic, it is helpful to inject additional position information into the LLM.\n",
        "\n",
        "To achive this, we can use two broad categories of position-aware embeddings: relative positional embeddings and absolute positional embeddings. Absolute positional embeddings are directly associated with specific positions in a sequence. For each position in the input sequence, a unique embedding is added to the token's embedding to convey its exact location. For instance, the first token will have a specific positional embedding, the second token another distinct embedding, and so on.\n",
        "\n",
        "Instead of focusing on the absolute position of a token, the emphasis of relative positional embeddings is on the relative position or distance between tokens. This means the model learns the relationships in terms of \"how far apart\" rather than \"at which exact position\". The advantage here is that the model can generalize better to sequences of varying lengths, even if it hasn't seen such lengths during training.\n",
        "\n",
        "Both types of positional embeddings aim to augment the capacity of LLMS to understand the order and relationships between tokens, ensuring more accurate and context-aware predictions. The choice between them often depends on the specific application and the nature of the data being processed."
      ],
      "metadata": {
        "id": "hIckPhPfbYFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(f'Token IDs:\\n{inputs}\\n'\n",
        "      f'Targets:\\n{targets}\\n')\n",
        "\n",
        "# data batch consists of 8 text samples and tokens each\n",
        "print(f'Inputs shape: {inputs.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCnIom8ZbkZH",
        "outputId": "91a3ef84-b999-4b24-a969-cb21b9934a34"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            "tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "Targets:\n",
            "tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n",
            "\n",
            "Inputs shape: torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use embedding layer to embed these token IDs\n",
        "# into 256-dimensional vectors\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "# 8x4x256: each token ID embeded as a 256-dimensional vector\n",
        "token_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIQszKqvk7A1",
        "outputId": "f607386d-59dc-4fe7-951b-a255c2a96129"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create an absolute embedding layer with\n",
        "# the same embedding dimension as token_embedding_layer\n",
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length,\n",
        "                                         output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "pos_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M0rX5KImzT-",
        "outputId": "15aacdbb-d46e-49f1-a249-f46ef11749f8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tHqyxUbm9fi",
        "outputId": "9a17801c-c8db-4651-ffaf-a8fc2bc8d7df"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1465, -0.0941, -0.2197,  ...,  0.6289,  0.3811, -1.1052],\n",
              "        [ 1.0556, -0.1906, -0.3616,  ...,  0.4743, -0.4925, -1.7694],\n",
              "        [-1.6037, -0.0578, -0.7502,  ...,  1.1309,  0.5436,  1.1704],\n",
              "        [-0.0675,  0.9043,  1.0599,  ..., -1.8826,  0.1242, -1.4644]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "input_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKQ9Ux1y3G4V",
        "outputId": "2b67f6c4-c435-43c1-bf51-876176b69caa"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "* LLMs require textual data to be converted into numerical vectors, known as embeddings, since they can't process raw text. Embeddings transform discrete data (like words or images) into continuous vector spaces, making them compatible with neural network operations.\n",
        "\n",
        "* As the first step, raw text is broken into tokens, which can be words or characters. Then, the tokens are converted into integer representations, termed token IDs.\n",
        "\n",
        "* Special tokens, such as $<|unk|>$ and $<|endoftext|>$, can be added to enhance the models understanding and handle various contexts, such as unknown words or making the boundary between unrelated texts.\n",
        "\n",
        "* The pyte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3 can efficiently handle unknown words by breaking them down into subword units or individual characters.\n",
        "\n",
        "* We use a sliding window approach on tokenized data to generate input-target pairs for LLM training.\n",
        "\n",
        "* Embedding layers in PyTorch function as a  lookup operation, retrieving vectors corresponding to token IDs. The resulting embedding vectors provide continuous representations of tokens, which is crucial for training deep learning models like LLMs.\n",
        "\n",
        "* While token embeddings provide consistent vector representations for each token, they lack a sense of the token's position in a sequence. to rectify this, two main types of positional embeddings exist: absoute and relative.\n",
        "\n",
        "**Procedure**: input text -> broken into individual words/characters -> converted into token IDs using a vocabulary -> converted into embedding vectors -> added with positional embeddings of a similar size -> input embeddings as input for main LLM layers."
      ],
      "metadata": {
        "id": "4JMcLDux3udS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKcMVoph6SQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}